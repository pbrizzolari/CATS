# Plot
itemFrequencyPlot(dataset_courses, topN = 10)
# Visualising the results
inspect(sort(associa_rules, by = 'lift')[1:10])
plot(associa_rules, method = "graph",
measure = "confidence", shading = "lift")
# Visualising the results
inspect(sort(associa_rules, by = 'lift'))
plot(associa_rules, method = "graph",
measure = "confidence", shading = "lift")
associa_rules = apriori(data = dataset_courses[,1:3],
parameter = list(support = 0.01,
confidence = 0.5))
# Plot
itemFrequencyPlot(dataset_courses, topN = 10)
# Visualising the results
inspect(sort(associa_rules, by = 'lift'))
plot(associa_rules, method = "graph",
measure = "confidence", shading = "lift")
associa_rules = apriori(data = dataset_courses,
parameter = list(support = 0.01,
confidence = 0.5))
# Plot
itemFrequencyPlot(dataset_courses, topN = 10)
# Visualising the results
inspect(sort(associa_rules, by = 'lift'))
plot(associa_rules, method = "graph",
measure = "confidence", shading = "lift")
associa_rules = apriori(data = dataset_courses[,1:2],
parameter = list(support = 0.01,
confidence = 0.5))
# Plot
itemFrequencyPlot(dataset_courses, topN = 10)
# Visualising the results
inspect(sort(associa_rules, by = 'lift'))
plot(associa_rules, method = "graph",
measure = "confidence", shading = "lift")
dataset_courses[,1:2]
associa_rules = apriori(data = dataset_courses[,1:3],
parameter = list(support = 0.01,
confidence = 0.5))
# Plot
itemFrequencyPlot(dataset_courses, topN = 10)
# Visualising the results
inspect(sort(associa_rules, by = 'lift'))
plot(associa_rules, method = "graph",
measure = "confidence", shading = "lift")
image(dataset_courses)
# Plot
itemFrequencyPlot(dataset_courses, topN = 10)
table(dataset_courses)
summary(dataset_courses)
dataset_courses <- data %>%
select(ml,ir,db,stats)
summary(dataset_courses)
names(c(ml,ir,db,stats))
dataset_courses <- data %>%
select(ml,ir,db,stats) %>%
filter(ir!="unknown", stats !="unknown", db!="unknown")
dataset_courses <- as(dataset_courses, "transactions")
image(dataset_courses)
# Fitting model
# Training Apriori on the dataset
associa_rules = apriori(data = dataset_courses,
parameter = list(support = 0.01,
confidence = 0.5))
# Plot
itemFrequencyPlot(dataset_courses, topN = 10)
# Visualising the results
inspect(sort(associa_rules, by = 'lift'))
plot(associa_rules, method = "graph",
measure = "confidence", shading = "lift")
# what led to have not followed an ir course
associa_rules_ir <-  apriori(data = dataset_courses,
parameter = list(support = 0.01, confidence = 0.5),
appearance = list (rhs="ir=0"), control = list (verbose=F))
rules_conf <- sort(associa_rules_ir, by="confidence", decreasing=TRUE) # 'high-confidence' rules.
inspect(head(rules_conf))
plot(associa_rules_ir, method = "graph",
measure = "confidence", shading = "lift")
# what led to have not followed an ir course
associa_rules_ir <-  apriori(data = dataset_courses,
parameter = list(support = 0.01, confidence = 0.5),
appearance = list (rhs="ir=0"), control = list (verbose=F))
rules_conf <- sort(associa_rules_ir, by="confidence", decreasing=TRUE) # 'high-confidence' rules.
inspect(head(rules_conf))
plot(associa_rules_ir, method = "graph",
measure = "confidence", shading = "lift")
# what led to have not followed an ir course
associa_rules_ir <-  apriori(data = dataset_courses,
parameter = list(support = 0.2, confidence = 0.5),
appearance = list (rhs="ir=0"), control = list (verbose=F))
rules_conf <- sort(associa_rules_ir, by="confidence", decreasing=TRUE) # 'high-confidence' rules.
inspect(head(rules_conf))
plot(associa_rules_ir, method = "graph",
measure = "confidence", shading = "lift")
plot(associa_rules_ir)
plot(associa_rules_ir, method = "paracoord",
measure = "confidence", shading = "lift")
# what not following an ir course lead to
associa_rules_ir <-  apriori(data = dataset_courses,
parameter = list(support = 0.01, confidence = 0.5),
appearance = list (lhs="ir=1"), control = list (verbose=F))
rules_conf <- sort(associa_rules_ir, by="confidence", decreasing=TRUE) # 'high-confidence' rules.
inspect(head(rules_conf))
# ir=0 led to follow what other courses
associa_rules_ir <-  apriori(data = dataset_courses,
parameter = list(support = 0.2, confidence = 0.5),
appearance = list lhs="ir=0"), control = list (verbose=F))
rules_conf <- sort(associa_rules_ir, by="confidence", decreasing=TRUE) # 'high-confidence' rules.
inspect(head(rules_conf))
# ir=0 led to follow what other courses
associa_rules_ir <-  apriori(data = dataset_courses,
parameter = list(support = 0.2, confidence = 0.5),
appearance = list(default="lhs", lhs="ir=0"), control = list (verbose=F))
rules_conf <- sort(associa_rules_ir, by="confidence", decreasing=TRUE) # 'high-confidence' rules.
inspect(head(rules_conf))
rules_conf
# ir=0 led to follow what other courses
associa_rules_ir <-  apriori(data = dataset_courses,
parameter = list(support = 0.02, confidence = 0.5),
appearance = list(default="lhs", lhs="ir=0"), control = list (verbose=F))
rules_conf <- sort(associa_rules_ir, by="confidence", decreasing=TRUE) # 'high-confidence' rules.
inspect(head(rules_conf))
# what other courses where followed for having ir=0
associa_rules_ir <-  apriori(data = dataset_courses,
parameter = list(support = 0.2, confidence = 0.5),
appearance = list (rhs="ir=0"), control = list (verbose=F))
rules_conf <- sort(associa_rules_ir, by="confidence", decreasing=TRUE) # 'high-confidence' rules.
inspect(head(rules_conf))
# what other courses where followed for having stats = yes
associa_rules_ir <-  apriori(data = dataset_courses,
parameter = list(support = 0.2, confidence = 0.5),
appearance = list (rhs="stats = yes"), control = list (verbose=F))
# what other courses where followed for having stats = yes
associa_rules_ir <-  apriori(data = dataset_courses,
parameter = list(support = 0.2, confidence = 0.5),
appearance = list (rhs="stats = mu"), control = list (verbose=F))
# what other courses where followed for having stats = yes
associa_rules_ir <-  apriori(data = dataset_courses,
parameter = list(support = 0.2, confidence = 0.5),
appearance = list (default="lhs",rhs="stats = mu"), control = list (verbose=F))
# what other courses where followed for having stats = yes
associa_rules_ir <-  apriori(data = dataset_courses,
parameter = list(support = 0.2, confidence = 0.5),
appearance = list (default="lhs", rhs="ml=yes"), control = list (verbose=F))
rules_conf <- sort(associa_rules_ir, by="confidence", decreasing=TRUE) # 'high-confidence' rules.
inspect(head(rules_conf))
plot(associa_rules_ir, method = "graph",
measure = "confidence", shading = "lift")
pie(table(stats), labels = names(stats))
hist(age)
associa_rules_ir <-  apriori(data = dataset_courses,
parameter = list(support = 0.2, confidence = 0.5),
appearance = list (default="lhs", rhs="ml=yes"), control = list (verbose=F))
rules_conf <- sort(associa_rules_ir, by="confidence", decreasing=TRUE) # 'high-confidence' rules.
inspect(head(rules_conf))
plot(associa_rules_ir, method = "graph",
measure = "confidence", shading = "lift")
plot(associa_rules_ir, method = "paracoord",
measure = "confidence", shading = "lift")
plot(associa_rules_ir, method = "graph",
measure = "confidence", shading = "lift")
##### stress and courses ####
dataset_courses <- data %>%
select(ml,ir,db,stats, stress) %>%
mutate(stress_level = case_when(
stress >= 0 & stress <=33 ~ "low",
(stress > 33 & stress <=66) ~ "mid",
stress > 66 & stress <=100 ~ "high",
is.na(stress) ~ "NA",
TRUE ~ "Other"
)) %>%
filter(ir!="unknown", stats !="unknown", db!="unknown")
dataset_courses
dataset_courses <- as(dataset_courses, "transactions")
# Fitting model
# Training Apriori on the dataset
associa_rules = apriori(data = dataset_courses,
parameter = list(support = 0.01,
confidence = 0.5))
itemFrequencyPlot(dataset_courses, topN = 10)
# Visualising the results
inspect(sort(associa_rules, by = 'lift'))
# Visualising the results
head(inspect(sort(associa_rules, by = 'lift')))
# Visualising the results
inspect(head(sort(associa_rules, by = 'lift')))
# Visualising the results
inspect(head(sort(associa_rules, by = 'lift')))
# Fitting model
# Training Apriori on the dataset
associa_rules = apriori(data = dataset_courses,
parameter = list(support = 0.01, confidence = 0.5),
appearance = list (default="lhs", rhs="stress=[30.7,67.7)"), control = list (verbose=F))
itemFrequencyPlot(dataset_courses, topN = 10)
# Visualising the results
inspect(head(sort(associa_rules, by = 'lift')))
dataset_courses <- data %>%
mutate(stress_level = case_when(
stress >= 0 & stress <=33 ~ "low",
(stress > 33 & stress <=66) ~ "mid",
stress > 66 & stress <=100 ~ "high",
is.na(stress) ~ "NA",
TRUE ~ "Other"
)) %>%
filter(ir!="unknown", stats !="unknown", db!="unknown")  %>%
select(ml,ir,db,stats, stress_level)
dataset_courses <- as(dataset_courses, "transactions")
# Fitting model
# Training Apriori on the dataset
associa_rules = apriori(data = dataset_courses,
parameter = list(support = 0.01, confidence = 0.5),
appearance = list (default="lhs", rhs="stress=[30.7,67.7)"), control = list (verbose=F))
itemFrequencyPlot(dataset_courses, topN = 10)
# Visualising the results
inspect(head(sort(associa_rules, by = 'lift')))
# Fitting model
# Training Apriori on the dataset
associa_rules = apriori(data = dataset_courses,
parameter = list(support = 0.01, confidence = 0.5),
appearance = list (default="lhs", rhs="stress_level=mid"), control = list (verbose=F))
itemFrequencyPlot(dataset_courses, topN = 10)
# Visualising the results
inspect(head(sort(associa_rules, by = 'lift')))
associa_rules = apriori(data = dataset_courses,
parameter = list(support = 0.01, confidence = 0.5)
)
# Visualising the results
inspect(head(sort(associa_rules, by = 'lift')))
# Visualising the results
inspect(head(sort(associa_rules, by = 'lift')))$rhs
?inspect
# Visualising the results
inspect(head(sort(associa_rules, by = 'lift')))[,2]
# Visualising the results
inspect(head(sort(associa_rules, by = 'confidence')))
associa_rules = apriori(data = dataset_courses,
parameter = list(support = 0.01, confidence = 0.5),
appearance = list(default="lhs", rhs=c("stress_level=low", "stress_level=mid", "stress_level=high")))
# Visualising the results
inspect(head(sort(associa_rules, by = 'confidence')))
# Visualising the results
inspect(sort(associa_rules, by = 'confidence'))
plot(associa_rules, method = "graph",
measure = "confidence", shading = "lift")
spotify_songs <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-21/spotify_songs.csv')
summary(spotify_songs)
fi1 <- lm(track_popularity~playlist_genre+playlist_subgenre+
danceability+energy+key+loudness+mode+speechiness+acousticness+instrumentalness+
liveness+valence+tempo+duration_ms, data=spotify_songs)
summary(fit1)
fit1 <- lm(track_popularity~playlist_genre+playlist_subgenre+
danceability+energy+key+loudness+mode+speechiness+acousticness+instrumentalness+
liveness+valence+tempo+duration_ms, data=spotify_songs)
summary(fit1)
fit1 <- lm(track_popularity~playlist_genre+
danceability+energy+key+loudness+mode+speechiness+acousticness+instrumentalness+
liveness+valence+tempo+duration_ms, data=spotify_songs)
summary(fit1)
fit2 <- lm(track_popularity~playlist_genre+
danceability+energy+key+loudness+speechiness+acousticness+instrumentalness+
liveness+valence+tempo+duration_ms, data=spotify_songs)
summary(fit2)
fit3 <- lm(track_popularity~playlist_genre+
danceability+energy+loudness+speechiness+acousticness+instrumentalness+
liveness+valence+tempo+duration_ms, data=spotify_songs)
summary(fit3)
fit4 <- lm(track_popularity~playlist_genre+
danceability+energy+loudness+speechiness+acousticness+instrumentalness+
liveness+tempo+duration_ms, data=spotify_songs)
summary(fit4)
fit5 <- lm(track_popularity~playlist_genre+
danceability+energy+loudness+acousticness+instrumentalness+
liveness+tempo+duration_ms, data=spotify_songs)
summary(fit5)
predict(fit5)
plot(predict(fit5))
fitted(fit5)
plot(fitted(fit5))
?predict
detach(spotify_songs)
qqnorm(spotify_songs$track_popularity[track_popularity!=0])
qqnorm(spotify_songs$track_popularity[spotify_songs$track_popularity!=0])
qqline(spotify_songs$track_popularity[spotify_songs$track_popularity!=0])
summary(fit5)
plot(fitted(fit5), track_popularity)
plot(fitted(fit5), spotify_songs$track_popularity)
abline()
abline(y=x, col="red")
?abline
abline(a,b, col="red")
fit1 <- lm(track_popularity~playlist_genre+
danceability+energy+key+loudness+mode+speechiness+acousticness+instrumentalness+
liveness+valence+tempo+duration_ms, data=spotify_songs)
# loading data
spotify_songs <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-21/spotify_songs.csv')
# split dataset
library(caret)
indxTrain <- createDataPartition(y = spotify_songs$track_popularity, p = 0.6, list = FALSE)
training <- spotify_songs[indxTrain,]
testing <- spotify_songs[-indxTrain,]
prop.table(table(training$Subgroup))
prop.table(table(training$track_popularity))
hist(training$track_popularity)
hist(training$track_popularity, prob=T)
View(training)
per(mfrow=c(3,1))
hist(training$track_popularity, prob=T)
par(mfrow=c(3,1))
hist(training$track_popularity, prob=T)
hist(testing$track_popularity, prob=T)
hist(spotify_songs$track_popularity, prob=T)
par(mfrow=c(1,1))
pred <- predict(fit, newdata=testing)
fit <- lm(track_popularity~playlist_genre+
danceability+energy+loudness+acousticness+instrumentalness+
liveness+tempo+duration_ms, data=training)
pred <- predict(fit, newdata=testing)
sqrt(mean((pred-testing$track_popularity)^2))
plot(pred)
points(testing$track_popularity)
plot(pred)
points(testing$track_popularity, col=2)
library(tidyverse)
library(rstudioapi)
library(ggfortify)
library(caret)
setwd(dirname(getActiveDocumentContext()$path))
data <- read.csv2("merged.csv", header = T, sep=";", row.names = 1)
data[,1]
set.seed(123)
indxTrain <- createDataPartition(y = data[,1], p = 0.6, list = FALSE)
training <- data[indxTrain,]
testing <- data[-indxTrain,]
prop.table(table(training$Subgroup))
prop.table(table(testing$Subgroup))
prop.table(table(data$Subgroup))
tuneGrid <- data.frame(
.mtry = seq(10, dim(training)[2], by = 200),
.splitrule = "gini",
.min.node.size = 5
)
model <- train(
Subgroup~.,
tuneGrid = tuneGrid,
data = training,
method = "ranger",
trControl = trainControl(
method = "repeatedcv",
number = 5, # validate
repeats = 1, # inner
verboseIter = TRUE
)
)
model
plot(model)
model$bestTune
model$bestTune[1]
model
pred <- predict(model, newdata = testing)
confusionMatrix(pred, as.factor(testing$Subgroup))
confus <- confusionMatrix(pred, as.factor(testing$Subgroup))
confus$table
confus$mode
confus$positive
confus$byClass
confus$ov
confus$overall
confus$overall[1]
sim <- 100
model_sim <- array(NA, sim)
model_acc <- array(NA, sim)
for (i in 1:sim){
data_fs <- feature_selection(data, data[,2:dim(data)[2]], data[,1], perc = 0.65)
indxTrain <- createDataPartition(y = data[,1], p = 0.7, list = FALSE)
training <- data[indxTrain,]
testing <- data[-indxTrain,]
model <- train(
Subgroup~.,
tuneGrid = tuneGrid,
data = training,
method = "ranger",
trControl = trainControl(
method = "repeatedcv",
number = 5, # validate
repeats = 10, # inner
verboseIter = TRUE
)
)
print(i)
#plot(model)
pred <- predict(model, newdata = testing)
confus <- confusionMatrix(pred, as.factor(testing$Subgroup))
model_sim[i] <- model$bestTune[1]
model_acc[i] <- confus$overall[1]
}
########## feature selection ##########
feature_selection <- function(data, X, y, perc = 0.6){
fs_matrix <- filterVarImp(X, y)
fs <- apply(fs_matrix,1,mean)
sorted_features <- sort(fs, decreasing = T)
n <- length(sorted_features[sorted_features >= perc])
selected <- sorted_features[1:n]
keep <- c(names(data[,1:2]), names(selected))
data_fs <- data[, (names(data) %in% keep)]
return(data_fs)
}
tuneGrid <- data.frame(
.mtry = seq(10, dim(training)[2], by = 200),
.splitrule = "gini",
.min.node.size = 5
)
sim <- 100
model_sim <- array(NA, sim)
model_acc <- array(NA, sim)
for (i in 1:sim){
data_fs <- feature_selection(data, data[,2:dim(data)[2]], data[,1], perc = 0.65)
indxTrain <- createDataPartition(y = data[,1], p = 0.7, list = FALSE)
training <- data[indxTrain,]
testing <- data[-indxTrain,]
model <- train(
Subgroup~.,
tuneGrid = tuneGrid,
data = training,
method = "ranger",
trControl = trainControl(
method = "repeatedcv",
number = 5, # validate
repeats = 10, # inner
verboseIter = TRUE
)
)
print(i)
#plot(model)
pred <- predict(model, newdata = testing)
confus <- confusionMatrix(pred, as.factor(testing$Subgroup))
model_sim[i] <- model$bestTune[1]
model_acc[i] <- confus$overall[1]
}
sim <- 100
model_sim <- array(NA, sim)
model_acc <- array(NA, sim)
for (i in 1:sim){
data_fs <- feature_selection(data, data[,2:dim(data)[2]], as.factor(data[,1]), perc = 0.65)
indxTrain <- createDataPartition(y = data[,1], p = 0.7, list = FALSE)
training <- data[indxTrain,]
testing <- data[-indxTrain,]
model <- train(
Subgroup~.,
tuneGrid = tuneGrid,
data = training,
method = "ranger",
trControl = trainControl(
method = "repeatedcv",
number = 5, # validate
repeats = 10, # inner
verboseIter = TRUE
)
)
print(i)
#plot(model)
pred <- predict(model, newdata = testing)
confus <- confusionMatrix(pred, as.factor(testing$Subgroup))
model_sim[i] <- model$bestTune[1]
model_acc[i] <- confus$overall[1]
}
?train
tuneGrid <- data.frame(
.mtry = seq(10, dim(training)[2], by = 300),
.splitrule = "gini",
.min.node.size = 5
)
sim <- 20
model_sim <- array(NA, sim)
model_acc <- array(NA, sim)
for (i in 1:sim){
data_fs <- feature_selection(data, data[,2:dim(data)[2]], as.factor(data[,1]), perc = 0.65)
indxTrain <- createDataPartition(y = data[,1], p = 0.7, list = FALSE)
training <- data[indxTrain,]
testing <- data[-indxTrain,]
model <- train(
Subgroup~.,
tuneGrid = tuneGrid,
data = training,
method = "ranger",
trControl = trainControl(
method = "repeatedcv",
number = 5, # validate
repeats = 10, # inner
verboseIter = TRUE
)
)
print(i)
#plot(model)
pred <- predict(model, newdata = testing)
confus <- confusionMatrix(pred, as.factor(testing$Subgroup))
model_sim[i] <- model$bestTune[1]
model_acc[i] <- confus$overall[1]
}
boxplot(model_acc)
boxplot(model_sim)
model_sim
as.array(model_sim)
boxplot(as.array(model_sim))
unlist(model_sim)
boxplot(unlist(model_sim))
hist(unlist(model_sim))
table(unlist(model_sim))
barplot(unlist(model_sim))
barchart(unlist(model_sim))
hist(unlist(model_sim), nclass=8)
